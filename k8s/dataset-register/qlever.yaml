apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: dataset-register-qlever
  annotations:
    reconcile.fluxcd.io/forceAt: "2026-01-23T20:15:00Z"
spec:
  serviceAccountName: nde
  interval: 30m
  upgrade:
    remediation:
      remediateLastFailure: false
  chart:
    spec:
      chart: helm/nde-app
      reconcileStrategy: Revision
      sourceRef:
        kind: GitRepository
        name: nde
  values:
    workload:
      type: statefulset

    shareProcessNamespace: true

    containers:
      - name: app
        image:
          repository: adfreiburg/qlever
#          tag: REBUILD-INDEX-BETA
          tag: commit-27eef33
        flux:
          enabled: false
        port: 7001
        env:
          - name: ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                name: dataset-register-qlever
                key: access-token
        resources:
          # requests = limits for Guaranteed QoS.
          requests:
            cpu: "2"
            memory: 12Gi
          limits:
            cpu: "2"
            memory: 12Gi
        startupProbe:
          httpGet:
            path: /?query=ASK%20%7B%7D
            port: 7001
          initialDelaySeconds: 5
          periodSeconds: 10
          failureThreshold: 60  # Allow time for indexing.
        livenessProbe:
          httpGet:
            path: /?query=ASK%20%7B%7D
            port: 7001
          timeoutSeconds: 5
          failureThreshold: 10
        volumeMounts:
          - name: data
            mountPath: /data
          - name: config
            mountPath: /data/Qleverfile
            subPath: Qleverfile
        command:
          - sh
          - -c
          - |
            # App container owns the server lifecycle.
            # Rebuild sidecar kills ServerMain; this loop detects and restarts from newest dir.
            while true; do
              INDEX_DIR=$(ls -d /data/rebuild.* 2>/dev/null | sort -r | head -1)

              if [ -n "$INDEX_DIR" ]; then
                echo "Starting from rebuild dir: $INDEX_DIR"
                cd "$INDEX_DIR"
              else
                echo "No rebuild dir, building initial index..."
                cd /data
                qlever index --overwrite-existing
              fi

              echo "Starting QLever from $(pwd)"
              qlever start --access-token "$ACCESS_TOKEN"

              # Tail logs in background so they appear in Kubernetes logs.
              qlever log &

              # Block while server is running
              while pgrep -f ServerMain > /dev/null; do
                sleep 10
              done

              # Kill background log tail.
              pkill -f "tail.*server-log" || true

              echo "Server exited, restarting..."
              sleep 2
            done
      - name: s3-sync
        image:
          repository: d3fk/s3cmd
          tag: latest
        flux:
          enabled: false
        env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: dataset-register-s3
                key: key
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: dataset-register-s3
                key: secret
        volumeMounts:
          - name: data
            mountPath: /data
            readOnly: true
        command:
          - sh
          - -c
          - |
            while true; do
              if [ -f /data/dataset-register.update-triples ]; then
                echo "$(date): Uploading update-triples to S3..."
                s3cmd --host=ams3.digitaloceanspaces.com \
                  --host-bucket="%(bucket)s.ams3.digitaloceanspaces.com" \
                  put /data/dataset-register.update-triples s3://dataset-register/
              fi
              sleep 3600
            done
#      - name: rebuild
#        image:
#          repository: adfreiburg/qlever
#          tag: REBUILD-INDEX-BETA
#        flux:
#          enabled: false
#        env:
#          - name: ACCESS_TOKEN
#            valueFrom:
#              secretKeyRef:
#                name: dataset-register-qlever
#                key: access-token
#        volumeMounts:
#          - name: data
#            mountPath: /data
#          - name: config
#            mountPath: /data/Qleverfile
#            subPath: Qleverfile
#        command:
#          - sh
#          - -c
#          - |
#            while true; do
#              # Calculate seconds until 2:30 AM
#              now=$(date +%s)
#              today_target=$(date -d "today 02:30" +%s 2>/dev/null || date -v2H -v30M -v0S +%s)
#              if [ $now -lt $today_target ]; then
#                sleep_seconds=$((today_target - now))
#              else
#                tomorrow_target=$(date -d "tomorrow 02:30" +%s 2>/dev/null || date -v+1d -v2H -v30M -v0S +%s)
#                sleep_seconds=$((tomorrow_target - now))
#              fi
#
#              echo "Sleeping until 2:30 AM ($sleep_seconds seconds)..."
#              sleep $sleep_seconds
#
#              echo "Starting index rebuild at $(date)"
#
#              # Find and cd to latest rebuild dir (to read that index), or /data if none.
#              INDEX_DIR=$(ls -d /data/rebuild.* 2>/dev/null | sort -r | head -1)
#              cd "${INDEX_DIR:-/data}"
#
#              # Create new rebuild in /data (not nested).
#              NEW_DIR="/data/rebuild.$(date +%Y-%m-%dT%H:%M:%S)"
#              if ! qlever rebuild-index --access-token "$ACCESS_TOKEN" --index-dir "$NEW_DIR"; then
#                echo "ERROR: rebuild-index failed"
#                continue
#              fi
#
#              # Kill server; app container detects and restarts from newest dir.
#              echo "Killing server to trigger restart from $NEW_DIR"
#              pkill -f ServerMain || true
#
#              # Cleanup rebuild directories older than 7 days.
#              find /data -maxdepth 1 -name 'rebuild.*' -type d -mtime +7 -exec rm -rf {} \;
#
#              echo "Rebuild complete at $(date)"
#            done

    securityContext:
      fsGroup: 100

    volumes:
      - name: config
        configMap:
          name: dataset-register-qlever-qleverfile

    persistentVolumes:
      - name: data
        size: 50Gi

    ingresses:
      - hosts:
          - datasetregister.netwerkdigitaalerfgoed.nl
        paths:
          - path: /sparql
            pathType: Prefix
        tls:
          enabled: false

    configMaps:
      - name: qleverfile
        data:
          Qleverfile: |
            [data]
            NAME              = dataset-register
            GET_DATA_CMD      = curl -sS -O https://dataset-register.ams3.digitaloceanspaces.com/cleaned-up-registrations.nq
            TEXT_DESCRIPTION  = All literals, search with FILTER CONTAINS(?var, "...")
            DESCRIPTION       = NDE Dataset Register
            FORMAT            = nq

            [index]
            INPUT_FILES     = cleaned-up-registrations.nq
            CAT_INPUT_FILES = cat ${INPUT_FILES}
            SETTINGS_JSON   = { "ascii-prefixes-only": true, "num-triples-per-batch": 100000 }
            TEXT_INDEX      = none

            [server]
            PORT               = 7001
            MEMORY_FOR_QUERIES = 5G
            CACHE_MAX_SIZE     = 2G
            TIMEOUT            = 30s
            PERSIST_UPDATES    = true

            [runtime]
            SYSTEM = native

            [ui]
            UI_CONFIG = dataset-register
