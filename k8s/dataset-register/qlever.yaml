apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: dataset-register-qlever
  annotations:
    reconcile.fluxcd.io/forceAt: "2026-01-10T21:50:00Z"
spec:
  serviceAccountName: nde
  interval: 30m
  chart:
    spec:
      chart: helm/nde-app
      reconcileStrategy: Revision
      sourceRef:
        kind: GitRepository
        name: nde
  values:
    workload:
      type: statefulset

    shareProcessNamespace: true

    containers:
      - name: app
        image:
          repository: adfreiburg/qlever
          tag: REBUILD-INDEX-BETA
          pullPolicy: Always
        flux:
          enabled: false
        port: 7001
        env:
          - name: ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                name: dataset-register-qlever
                key: access-token
        resources:
          # requests = limits for Guaranteed QoS.
          requests:
            cpu: "2"
            memory: 12Gi
          limits:
            cpu: "2"
            memory: 12Gi
        startupProbe:
          httpGet:
            path: /?query=ASK%20%7B%7D
            port: 7001
          initialDelaySeconds: 120
          periodSeconds: 30
          failureThreshold: 10  # Allow up to 7 minutes for indexing
        livenessProbe:
          httpGet:
            path: /?query=ASK%20%7B%7D
            port: 7001
          timeoutSeconds: 5
          failureThreshold: 10
        volumeMounts:
          - name: data
            mountPath: /data
          - name: config
            mountPath: /data/Qleverfile
            subPath: Qleverfile
        command:
          - sh
          - -c
          - |
            cd /data
            INDEX_DIR=$(ls -d /data/rebuild.* 2>/dev/null | sort -r | head -1)

            if [ -n "$INDEX_DIR" ]; then
              echo "Starting from rebuild dir: $INDEX_DIR"
              cd "$INDEX_DIR"
            else
              echo "No rebuild dir, building initial index..."
              qlever index --overwrite-existing
            fi

            qlever start --access-token "$ACCESS_TOKEN"

            # Log loop; restarts when sidecar kills qlever log after rebuild.
            while true; do
              INDEX_DIR=$(ls -d /data/rebuild.* 2>/dev/null | sort -r | head -1)
              cd "${INDEX_DIR:-/data}"
              echo "Tailing logs from $(pwd)"
              qlever log || true
            done
      - name: s3-sync
        image:
          repository: d3fk/s3cmd
          tag: latest
        flux:
          enabled: false
        env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: dataset-register-s3
                key: key
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: dataset-register-s3
                key: secret
        volumeMounts:
          - name: data
            mountPath: /data
            readOnly: true
        command:
          - sh
          - -c
          - |
            while true; do
              if [ -f /data/dataset-register.update-triples ]; then
                echo "$(date): Uploading update-triples to S3..."
                s3cmd --host=ams3.digitaloceanspaces.com \
                  --host-bucket="%(bucket)s.ams3.digitaloceanspaces.com" \
                  put /data/dataset-register.update-triples s3://dataset-register/
              fi
              sleep 3600
            done
      - name: rebuild
        image:
          repository: adfreiburg/qlever
          tag: REBUILD-INDEX-BETA
          pullPolicy: Always
        flux:
          enabled: false
        env:
          - name: ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                name: dataset-register-qlever
                key: access-token
        volumeMounts:
          - name: data
            mountPath: /data
          - name: config
            mountPath: /data/Qleverfile
            subPath: Qleverfile
        command:
          - sh
          - -c
          - |
            while true; do
              # Calculate seconds until 3 AM
              now=$(date +%s)
              today_3am=$(date -d "today 03:00" +%s 2>/dev/null || date -v3H -v0M -v0S +%s)
              if [ $now -lt $today_3am ]; then
                sleep_seconds=$((today_3am - now))
              else
                tomorrow_3am=$(date -d "tomorrow 03:00" +%s 2>/dev/null || date -v+1d -v3H -v0M -v0S +%s)
                sleep_seconds=$((tomorrow_3am - now))
              fi

              echo "Sleeping until 3 AM ($sleep_seconds seconds)..."
              sleep $sleep_seconds

              echo "Starting index rebuild at $(date)"

              # Find and cd to latest rebuild dir (to read that index), or /data if none
              INDEX_DIR=$(ls -d /data/rebuild.* 2>/dev/null | sort -r | head -1)
              cd "${INDEX_DIR:-/data}"

              # Create new rebuild in /data (not nested)
              NEW_DIR="/data/rebuild.$(date +%Y-%m-%dT%H:%M:%S)"
              if ! qlever rebuild-index --access-token "$ACCESS_TOKEN" --index-dir "$NEW_DIR"; then
                echo "ERROR: rebuild-index failed"
                continue
              fi
            
              cd "$NEW_DIR"
              qlever start --access-token "$ACCESS_TOKEN" --kill-existing-with-same-port

              # Signal app container to restart qlever log in new directory
              pkill -f "qlever log" || true

              # Cleanup rebuild directories older than 7 days
              find /data -maxdepth 1 -name 'rebuild.*' -type d -mtime +7 -exec rm -rf {} \;

              echo "Rebuild complete at $(date)"
            done

    securityContext:
      fsGroup: 100

    volumes:
      - name: config
        configMap:
          name: dataset-register-qlever-qleverfile

    persistentVolumes:
      - name: data
        size: 50Gi

    ingresses:
      - hosts:
          - datasetregister.netwerkdigitaalerfgoed.nl
        paths:
          - path: /sparql
            pathType: Prefix
        tls:
          enabled: false

    configMaps:
      - name: qleverfile
        data:
          Qleverfile: |
            [data]
            NAME              = dataset-register
            GET_DATA_CMD      = curl -sS -O https://dataset-register.ams3.digitaloceanspaces.com/cleaned-up-registrations.nq
            TEXT_DESCRIPTION  = All literals, search with FILTER CONTAINS(?var, "...")
            DESCRIPTION       = NDE Dataset Register
            FORMAT            = nq

            [index]
            INPUT_FILES     = cleaned-up-registrations.nq
            CAT_INPUT_FILES = cat ${INPUT_FILES}
            SETTINGS_JSON   = { "ascii-prefixes-only": true, "num-triples-per-batch": 100000 }
            TEXT_INDEX      = none

            [server]
            PORT               = 7001
            MEMORY_FOR_QUERIES = 5G
            CACHE_MAX_SIZE     = 2G
            TIMEOUT            = 30s
            PERSIST_UPDATES    = true

            [runtime]
            SYSTEM = native

            [ui]
            UI_CONFIG = dataset-register
