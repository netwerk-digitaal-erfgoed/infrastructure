apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: dataset-register-qlever
  annotations:
    reconcile.fluxcd.io/forceAt: "2026-01-23T20:15:00Z"
spec:
  serviceAccountName: nde
  interval: 30m
  upgrade:
    remediation:
      remediateLastFailure: false
  chart:
    spec:
      chart: helm/nde-app
      reconcileStrategy: Revision
      sourceRef:
        kind: GitRepository
        name: nde
  values:
    workload:
      type: statefulset

    containers:
      - name: app
        image:
          repository: adfreiburg/qlever
          tag: REBUILD-INDEX-BETA@sha256:ce9317e811a3c9892fa42d7a99ec0d3db856afb41749984f6145b3f125df8c7c
        flux:
          enabled: false
        port: 7001
        env:
          - name: ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                name: dataset-register-qlever
                key: access-token
        resources:
          # requests = limits for Guaranteed QoS.
          requests:
            cpu: "2"
            memory: 12Gi
          limits:
            cpu: "2"
            memory: 12Gi
        startupProbe:
          httpGet:
            path: /?query=ASK%20%7B%7D
            port: 7001
          initialDelaySeconds: 5
          periodSeconds: 10
          failureThreshold: 60  # Allow time for indexing.
        livenessProbe:
          httpGet:
            path: /?query=ASK%20%7B%7D
            port: 7001
          timeoutSeconds: 5
          failureThreshold: 10
        volumeMounts:
          - name: data
            mountPath: /data
          - name: config
            mountPath: /data/Qleverfile
            subPath: Qleverfile
        workingDir: /data
        command:
          - sh
          - -c
          - |
            # Start server
            qlever start --access-token "$ACCESS_TOKEN"

            # Tail logs in background so they appear in Kubernetes logs.
            qlever log &

            # Daily rebuild loop
            while true; do
              # Calculate seconds until 2:30 AM
              now=$(date +%s)
              today_target=$(date -d "today 02:30" +%s 2>/dev/null || date -v2H -v30M -v0S +%s)
              if [ $now -lt $today_target ]; then
                sleep_seconds=$((today_target - now))
              else
                tomorrow_target=$(date -d "tomorrow 02:30" +%s 2>/dev/null || date -v+1d -v2H -v30M -v0S +%s)
                sleep_seconds=$((tomorrow_target - now))
              fi

              echo "Sleeping until 2:30 AM ($sleep_seconds seconds)..."
              sleep $sleep_seconds

              echo "Starting index rebuild at $(date)"
              qlever rebuild-index --access-token "$ACCESS_TOKEN" --restart-when-finished --keep-old-index-dirs none || echo "ERROR: rebuild-index failed"

              echo "Rebuild complete at $(date)"
            done
      - name: s3-sync
        image:
          repository: d3fk/s3cmd
          tag: latest
        flux:
          enabled: false
        env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: dataset-register-s3
                key: key
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: dataset-register-s3
                key: secret
        volumeMounts:
          - name: data
            mountPath: /data
            readOnly: true
        command:
          - sh
          - -c
          - |
            while true; do
              if [ -f /data/dataset-register.update-triples ]; then
                echo "$(date): Uploading update-triples to S3..."
                s3cmd --host=ams3.digitaloceanspaces.com \
                  --host-bucket="%(bucket)s.ams3.digitaloceanspaces.com" \
                  put /data/dataset-register.update-triples s3://dataset-register/
              fi
              sleep 3600
            done
    securityContext:
      fsGroup: 100

    volumes:
      - name: config
        configMap:
          name: dataset-register-qlever-qleverfile

    persistentVolumes:
      - name: data
        size: 50Gi

    ingresses:
      - hosts:
          - datasetregister.netwerkdigitaalerfgoed.nl
        paths:
          - path: /sparql
            pathType: Prefix
        tls:
          enabled: false

    configMaps:
      - name: qleverfile
        data:
          Qleverfile: |
            [data]
            NAME              = dataset-register
            GET_DATA_CMD      = curl -sS -O https://dataset-register.ams3.digitaloceanspaces.com/cleaned-up-registrations.nq
            TEXT_DESCRIPTION  = All literals, search with FILTER CONTAINS(?var, "...")
            DESCRIPTION       = NDE Dataset Register
            FORMAT            = nq

            [index]
            INPUT_FILES     = cleaned-up-registrations.nq
            CAT_INPUT_FILES = cat ${INPUT_FILES}
            SETTINGS_JSON   = { "ascii-prefixes-only": true, "num-triples-per-batch": 100000 }
            TEXT_INDEX      = none

            [server]
            PORT               = 7001
            MEMORY_FOR_QUERIES = 5G
            CACHE_MAX_SIZE     = 2G
            TIMEOUT            = 30s
            PERSIST_UPDATES    = true

            [runtime]
            SYSTEM = native

            [ui]
            UI_CONFIG = dataset-register
